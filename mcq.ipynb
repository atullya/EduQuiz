{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c773ad84-b951-4a36-be75-e5233ffb954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00725779-254d-4b5c-ad11-c7a799ef40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1299ba4-c3f1-48e5-a18a-3525d82dc7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53c1194-7886-477b-9919-e9a446500e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SQuAD sample\n",
    "squad = load_dataset(\"squad\", split=\"train[:20]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2aa86c-540b-4122-9b55-0b7cc28f41f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e878c2e35534a55bef83feec1d748b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atulm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\atulm\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e94a153bf6f49e7b40c02a6ca9ba828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20305f80c9324f19929b6c11e7679495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c4be02a15748478d3fa57ccd6f8471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b04da80da39424a8db82fda6c25e3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad414da98834903a702e813da4702a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4beb3059-3f24-4866-ba71-8a6910e032c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NLTK based text preprocessing ---\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)  # keep only alphabets and spaces\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "# --- Compute Term Frequency (TF) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "865814bd-4002-4114-bf50-2822030ea498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute Term Frequency (TF) ---\n",
    "def compute_tf(tokens):\n",
    "    tf = {}\n",
    "    for word in tokens:\n",
    "        tf[word] = tf.get(word, 0) + 1\n",
    "    total = len(tokens)\n",
    "    for word in tf:\n",
    "        tf[word] /= total\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cda1ba5-9faa-4d1a-8628-ab865507f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute Inverse Document Frequency (IDF) ---\n",
    "def compute_idf(documents):\n",
    "    N = len(documents)\n",
    "    idf = {}\n",
    "    all_words = set(word for doc in documents for word in doc)\n",
    "    for word in all_words:\n",
    "        containing_docs = sum(1 for doc in documents if word in doc)\n",
    "        idf[word] = math.log(N / (1 + containing_docs))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dae965c-c1f9-4d69-9e79-1be6f3b24a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute TF-IDF ---\n",
    "def compute_tf_idf(tf, idf):\n",
    "    return {word: tf[word] * idf.get(word, 0.0) for word in tf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5fb6056-19af-4dff-ad32-bc3894b0936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cosine similarity ---\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    words = set(vec1.keys()).union(set(vec2.keys()))\n",
    "    v1 = np.array([vec1.get(w, 0.0) for w in words])\n",
    "    v2 = np.array([vec2.get(w, 0.0) for w in words])\n",
    "    dot = np.dot(v1, v2)\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    return dot / (norm1 * norm2) if norm1 and norm2 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9633d889-56ed-4186-987f-ac8eb854fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate distractors ---\n",
    "def generate_distractors(answer, context, idf):\n",
    "    answer_tokens = preprocess(answer)\n",
    "    context_tokens = preprocess(context)\n",
    "    answer_vec = compute_tf_idf(compute_tf(answer_tokens), idf)\n",
    "\n",
    "    distractors = []\n",
    "    used_words = set(answer_tokens)\n",
    "    for word in set(context_tokens):\n",
    "        if word in used_words:\n",
    "            continue\n",
    "        word_vec = compute_tf_idf(compute_tf([word]), idf)\n",
    "        sim = cosine_similarity(answer_vec, word_vec)\n",
    "        if 0.2 < sim < 0.8:\n",
    "            distractors.append((word, sim))\n",
    "\n",
    "    distractors = sorted(distractors, key=lambda x: -x[1])[:3]\n",
    "    return [w for w, _ in distractors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e56d2bb3-5a3c-4476-a82f-67c41a607671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate question using T5 ---\n",
    "def generate_question_t5(context, answer):\n",
    "    input_text = f\"generate question: {context} answer: {answer}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_ids = model.generate(input_ids, max_length=64)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# --- Prepare IDF from all contexts ---\n",
    "all_contexts = [preprocess(example[\"context\"]) for example in squad]\n",
    "idf = compute_idf(all_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a744bca-1dea-4278-a033-11ebfd066a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare IDF from all contexts ---\n",
    "all_contexts = [preprocess(example[\"context\"]) for example in squad]\n",
    "idf = compute_idf(all_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d7d9cd6-11d8-4c74-bee5-229964af4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Generate MCQs ---\n",
    "mcqs = []\n",
    "for example in squad:\n",
    "    context = example[\"context\"]\n",
    "    answer = example[\"answers\"][\"text\"][0]\n",
    "    if not answer.strip():\n",
    "        continue\n",
    "\n",
    "    distractors = generate_distractors(answer, context, idf)\n",
    "    if len(distractors) < 3:\n",
    "        continue\n",
    "\n",
    "    question = generate_question_t5(context, answer)\n",
    "    options = distractors + [answer]\n",
    "    random.shuffle(options)\n",
    "\n",
    "    mcqs.append({\n",
    "        \"question\": question,\n",
    "        \"options\": options,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "\n",
    "    if len(mcqs) >= 5:\n",
    "        break\n",
    "\n",
    "# --- Print sample MCQs ---\n",
    "for i, mcq in enumerate(mcqs):\n",
    "    print(f\"\\nQuestion {i+1}: {mcq['question']}\")\n",
    "    for j, opt in enumerate(mcq['options']):\n",
    "        label = chr(65 + j)\n",
    "        print(f\"  {label}. {opt}\")\n",
    "    print(f\"Answer: {mcq['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b58cb-0aa5-4c5c-b180-f95cedcb94b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7351fd-cdf0-454f-a12c-efa41fd33677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf07272-8361-4d3e-aaea-775ab6729c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42f51e-e7f1-48ad-acb3-73cfe5a314f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17c65c53-d0ed-47db-98a6-7e790432418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: What is the name of the village in which a mysterious tree stood in the square?\n",
      "  A. soon\n",
      "  B. Eldermere\n",
      "  C. quaint\n",
      "  D. saw\n",
      "Answer: Eldermere\n",
      "\n",
      "Question 2: Who felt an undeniable pull toward the shimmering fruit?\n",
      "  A. eager\n",
      "  B. Young Emma\n",
      "  C. believing\n",
      "  D. held\n",
      "Answer: Young Emma\n",
      "\n",
      "Question 3: What did the villagers call the tree?\n",
      "  A. gnarled\n",
      "  B. crisp\n",
      "  C. Shakespear\n",
      "  D. glimpse\n",
      "Answer: Shakespear\n",
      "\n",
      "Question 4: Who felt an undeniable pull toward the shimmering fruit?\n",
      "  A. Emma\n",
      "  B. life\n",
      "  C. reality\n",
      "  D. autumn\n",
      "Answer: Emma\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk, sent_tokenize\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load fine-tuned T5 model and tokenizer for question generation\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    tree = ne_chunk(tagged, binary=False)\n",
    "    entities = set()\n",
    "\n",
    "    for subtree in tree:\n",
    "        if hasattr(subtree, 'label') and subtree.label() in ['PERSON', 'ORGANIZATION', 'GPE', 'LOCATION', 'FACILITY', 'GSP']:\n",
    "            entity = ' '.join([token for token, pos in subtree.leaves()])\n",
    "            if len(entity) > 1:\n",
    "                entities.add(entity)\n",
    "    return list(entities)\n",
    "\n",
    "def compute_tf(tokens):\n",
    "    tf = {}\n",
    "    for word in tokens:\n",
    "        tf[word] = tf.get(word, 0) + 1\n",
    "    total = len(tokens)\n",
    "    for word in tf:\n",
    "        tf[word] /= total\n",
    "    return tf\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    return {word: tf[word] * idf.get(word, 0.0) for word in tf}\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    words = set(vec1.keys()).union(set(vec2.keys()))\n",
    "    v1 = np.array([vec1.get(w, 0.0) for w in words])\n",
    "    v2 = np.array([vec2.get(w, 0.0) for w in words])\n",
    "    dot = np.dot(v1, v2)\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    return dot / (norm1 * norm2) if norm1 and norm2 else 0.0\n",
    "\n",
    "def generate_distractors(answer, context, idf):\n",
    "    answer_tokens = preprocess(answer)\n",
    "    context_tokens = preprocess(context)\n",
    "    answer_vec = compute_tf_idf(compute_tf(answer_tokens), idf)\n",
    "\n",
    "    distractors = []\n",
    "    used_words = set([t.lower() for t in answer_tokens])\n",
    "    for word in set(context_tokens):\n",
    "        if word in used_words:\n",
    "            continue\n",
    "        if word in answer.lower() or answer.lower() in word:\n",
    "            continue\n",
    "        word_vec = compute_tf_idf(compute_tf([word]), idf)\n",
    "        sim = cosine_similarity(answer_vec, word_vec)\n",
    "        if 0.2 < sim < 0.8:\n",
    "            distractors.append((word, sim))\n",
    "\n",
    "    distractors = sorted(distractors, key=lambda x: -x[1])[:3]\n",
    "    return [w for w, _ in distractors]\n",
    "\n",
    "def get_relevant_sentence(text, answer):\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if answer.lower() in sent.lower():\n",
    "            return sent\n",
    "    return text\n",
    "\n",
    "def generate_question_t5(context, answer):\n",
    "    sentence = get_relevant_sentence(context, answer)\n",
    "    # Highlight the answer with <hl> tags (case-insensitive)\n",
    "    pattern = re.compile(re.escape(answer), re.IGNORECASE)\n",
    "    highlighted_sentence = pattern.sub(f\"<hl>{answer}</hl>\", sentence, count=1)\n",
    "    input_text = f\"generate question: {highlighted_sentence} answer: {answer}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_ids = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
    "    question = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return question\n",
    "\n",
    "def generate_mcqs_from_text(text, desired_mcq_count=5):\n",
    "    candidate_answers = extract_named_entities(text)\n",
    "\n",
    "    if not candidate_answers:\n",
    "        # fallback to most frequent words if no named entities\n",
    "        candidate_answers = preprocess(text)[:20]\n",
    "\n",
    "    idf = {word: 1.0 for word in preprocess(text)}  # dummy IDF for single doc\n",
    "\n",
    "    mcqs = []\n",
    "    used_answers = set()\n",
    "\n",
    "    for answer in candidate_answers:\n",
    "        if len(mcqs) >= desired_mcq_count:\n",
    "            break\n",
    "        if len(answer.split()) > 5 or len(answer) < 2:\n",
    "            continue\n",
    "        if answer.lower() in used_answers:\n",
    "            continue\n",
    "\n",
    "        distractors = generate_distractors(answer, text, idf)\n",
    "\n",
    "        # Add random distractors if not enough\n",
    "        if len(distractors) < 3:\n",
    "            context_tokens = set(preprocess(text))\n",
    "            random_distractors = list(context_tokens - set([answer.lower()]))\n",
    "            random.shuffle(random_distractors)\n",
    "            for w in random_distractors:\n",
    "                if w not in distractors and w.lower() != answer.lower():\n",
    "                    distractors.append(w)\n",
    "                if len(distractors) >= 3:\n",
    "                    break\n",
    "\n",
    "        if len(distractors) < 3:\n",
    "            continue\n",
    "\n",
    "        question = generate_question_t5(text, answer)\n",
    "        options = distractors[:3] + [answer]\n",
    "        random.shuffle(options)\n",
    "\n",
    "        mcqs.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "        used_answers.add(answer.lower())\n",
    "\n",
    "    return mcqs\n",
    "\n",
    "\n",
    "# ======= Example usage =======\n",
    "input_text = \"\"\"\n",
    "In the heart of the quaint village of Eldermere, a mysterious tree stood tall in the town square.\n",
    "Its gnarled branches bore fruits that resembled pears, but with an unusual twist: they seemed to\n",
    "shimmer with a golden hue. The villagers affectionately named it the \"Shakespear\" tree, believing it\n",
    "held magical properties.\n",
    "\n",
    "Legend had it that anyone who tasted a Shakespear would gain a glimpse into their future.\n",
    "Curiosity spread like wildfire, and soon, villagers flocked to the tree, eager for a taste of destiny.\n",
    "Young Emma, a spirited girl with dreams of becoming a writer, felt an undeniable pull toward the\n",
    "shimmering fruit.\n",
    "\n",
    "One crisp autumn morning, she approached the tree, heart racing. With a deep breath, she plucked a\n",
    "Shakespear and took a bite. Instantly, a whirlwind of visions enveloped her. She saw herself standing\n",
    "on a grand stage, the applause of a thousand voices echoing in her ears. In another glimpse, she wandered\n",
    "through enchanted forests, her stories coming to life.\n",
    "\n",
    "Determined to fulfill these dreams, Emma spent every spare moment writing. The villagers, inspired by\n",
    "her passion, began sharing their own tales. The square buzzed with creativity, and soon, Eldermere\n",
    "became a hub of storytelling.\n",
    "\n",
    "As the seasons changed, Emmaâ€™s words took flight. She published her first book, a collection of\n",
    "enchanting stories, and it captured the hearts of many beyond Eldermere. The Shakespear tree\n",
    "continued to stand, its golden pears glimmering, a reminder that dreams, when nurtured, could blossom\n",
    "into reality.\n",
    "\n",
    "And so, in the embrace of magic and creativity, the legacy of the Shakespear lived on, inspiring\n",
    "generations to reach for their dreams.\n",
    "\"\"\"\n",
    "\n",
    "mcqs = generate_mcqs_from_text(input_text, desired_mcq_count=5)\n",
    "\n",
    "for i, mcq in enumerate(mcqs):\n",
    "    print(f\"\\nQuestion {i+1}: {mcq['question']}\")\n",
    "    for j, opt in enumerate(mcq['options']):\n",
    "        print(f\"  {chr(65+j)}. {opt}\")\n",
    "    print(f\"Answer: {mcq['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5fcd4-9e41-43b6-9ea9-098fbe2dbb23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8c88f0-69c2-494d-9ddb-8b0fa04cf67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c841b1-86e9-42af-8180-f9f9f704e273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f967d50-3957-4b78-b19c-8aff88bb9e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3501037-8f8d-4308-bbfc-9c9f05917d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92230446-d08a-4cc3-9950-79fa067fb078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab2a00-c29c-4428-800d-035c9fa71d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b898ff-e0c0-47f1-8913-02cf7fae54de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde4e96-df32-4b62-85a1-4cbb98d8250c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7125b93f-492b-4dd4-af59-aaa53f385f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "def compute_tf(tokens):\n",
    "    tf = {}\n",
    "    for word in tokens:\n",
    "        tf[word] = tf.get(word, 0) + 1\n",
    "    total = len(tokens)\n",
    "    for word in tf:\n",
    "        tf[word] /= total\n",
    "    return tf\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    return {word: tf[word] * idf.get(word, 1.0) for word in tf}\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    words = set(vec1.keys()).union(set(vec2.keys()))\n",
    "    v1 = np.array([vec1.get(w, 0.0) for w in words])\n",
    "    v2 = np.array([vec2.get(w, 0.0) for w in words])\n",
    "    dot = np.dot(v1, v2)\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    return dot / (norm1 * norm2) if norm1 and norm2 else 0.0\n",
    "\n",
    "def generate_distractors(answer, context, idf):\n",
    "    answer_tokens = preprocess(answer)\n",
    "    context_tokens = preprocess(context)\n",
    "    answer_vec = compute_tf_idf(compute_tf(answer_tokens), idf)\n",
    "\n",
    "    distractors = []\n",
    "    used = set(answer_tokens)\n",
    "    for word in set(context_tokens):\n",
    "        if word in used or len(word) < 3:\n",
    "            continue\n",
    "        word_vec = compute_tf_idf(compute_tf([word]), idf)\n",
    "        sim = cosine_similarity(answer_vec, word_vec)\n",
    "        # Relax thresholds for more distractors\n",
    "        if 0.0 < sim < 0.7:\n",
    "            distractors.append((word, sim))\n",
    "\n",
    "    distractors = sorted(distractors, key=lambda x: -x[1])\n",
    "    # Return top 3 or less if not enough found\n",
    "    return [w for w, _ in distractors[:3]]\n",
    "\n",
    "def generate_question_t5(context, answer):\n",
    "    # Simplify prompt for better question generation or fallback\n",
    "    try:\n",
    "        input_text = f\"generate question: answer: {answer} context: {context}\"\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        output_ids = model.generate(input_ids, max_length=64)\n",
    "        question = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        if len(question.strip()) < 10:  # fallback if output is nonsense\n",
    "            raise ValueError\n",
    "        return question\n",
    "    except:\n",
    "        return f\"What is {answer}?\"\n",
    "\n",
    "def generate_mcqs(text, top_k=10):\n",
    "    tokens = preprocess(text)\n",
    "    tf = compute_tf(tokens)\n",
    "    idf = {word: 1.0 for word in tf}  # single doc\n",
    "    tfidf = compute_tf_idf(tf, idf)\n",
    "    sorted_keywords = sorted(tfidf.items(), key=lambda x: -x[1])\n",
    "    candidate_answers = [word for word, score in sorted_keywords[:top_k]]\n",
    "\n",
    "    mcqs = []\n",
    "    for answer in candidate_answers:\n",
    "        distractors = generate_distractors(answer, text, idf)\n",
    "        if len(distractors) < 2:  # accept 2 distractors for more MCQs\n",
    "            continue\n",
    "        question = generate_question_t5(text, answer)\n",
    "        options = distractors + [answer]\n",
    "        random.shuffle(options)\n",
    "        mcqs.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "        if len(mcqs) >= 3:  # generate at least 3 MCQs\n",
    "            break\n",
    "    return mcqs\n",
    "\n",
    "# Sample Text (same as yours)\n",
    "sample_text = \"\"\"\n",
    "In the heart of the quaint village of Eldermere, a mysterious tree stood tall in the town square. \n",
    "Its gnarled branches bore fruits that resembled pears, but with an unusual twist: they seemed to \n",
    "shimmer with a golden hue. The villagers affectionately named it the \"Shakespear\" tree, believing \n",
    "it held magical properties. Legend had it that anyone who tasted a Shakespear would gain a glimpse \n",
    "into their future. Curiosity spread like wildfire, and soon, villagers flocked to the tree, eager \n",
    "for a taste of destiny. Young Emma, a spirited girl with dreams of becoming a writer, felt an \n",
    "undeniable pull toward the shimmering fruit. One crisp autumn morning, she approached the tree, \n",
    "heart racing. With a deep breath, she plucked a Shakespear and took a bite. Instantly, a whirlwind \n",
    "of visions enveloped her. She saw herself standing on a grand stage, the applause of a thousand \n",
    "voices echoing in her ears. In another glimpse, she wandered through enchanted forests, her stories \n",
    "coming to life. Determined to fulfill these dreams, Emma spent every spare moment writing. The \n",
    "villagers, inspired by her passion, began sharing their own tales. The square buzzed with \n",
    "creativity, and soon, Eldermere became a hub of storytelling. As the seasons changed, Emmaâ€™s words \n",
    "took flight. She published her first book, a collection of enchanting stories, and it captured the \n",
    "hearts of many beyond Eldermere. The Shakespear tree continued to stand, its golden pears \n",
    "glimmering, a reminder that dreams, when nurtured, could blossom into reality.\n",
    "\"\"\"\n",
    "\n",
    "mcqs = generate_mcqs(sample_text)\n",
    "\n",
    "for i, mcq in enumerate(mcqs):\n",
    "    print(f\"\\nQuestion {i+1}: {mcq['question']}\")\n",
    "    for j, opt in enumerate(mcq['options']):\n",
    "        print(f\"  {chr(65+j)}. {opt}\")\n",
    "    print(f\"Answer: {mcq['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95db68f-97b6-400a-8eb6-78f758d96197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f71a1ea-ab6f-4496-83ba-a70380edd6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc14c9a-09cb-448b-8788-b0ffb3cb69d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337cd662-cb68-464b-bc1a-49bb618dbd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebe0a4cf-254d-4bfa-a71b-19c1d0377a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: What is the name of the village in which a mysterious tree stood in the square?\n",
      "  A. Eldermere\n",
      "  B. resembled\n",
      "  C. blossom\n",
      "  D. twist\n",
      "Answer: Eldermere\n",
      "\n",
      "Question 2: Who felt an undeniable pull toward the shimmering fruit?\n",
      "  A. Young Emma\n",
      "  B. enchanting\n",
      "  C. legend\n",
      "  D. every\n",
      "Answer: Young Emma\n",
      "\n",
      "Question 3: What did the villagers call the tree?\n",
      "  A. many\n",
      "  B. Shakespear\n",
      "  C. glimmering\n",
      "  D. whirlwind\n",
      "Answer: Shakespear\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk, sent_tokenize\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Download required NLTK data (only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the fine-tuned T5 model and tokenizer for SQuAD-style QG\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    tree = ne_chunk(tagged, binary=False)\n",
    "    entities = set()\n",
    "\n",
    "    for subtree in tree:\n",
    "        if hasattr(subtree, 'label') and subtree.label() in ['PERSON', 'ORGANIZATION', 'GPE', 'LOCATION', 'FACILITY']:\n",
    "            entity = ' '.join([token for token, pos in subtree.leaves()])\n",
    "            if len(entity) > 1:\n",
    "                entities.add(entity)\n",
    "    return list(entities)\n",
    "\n",
    "def compute_tf(tokens):\n",
    "    tf = {}\n",
    "    for word in tokens:\n",
    "        tf[word] = tf.get(word, 0) + 1\n",
    "    total = len(tokens)\n",
    "    for word in tf:\n",
    "        tf[word] /= total\n",
    "    return tf\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    return {word: tf[word] * idf.get(word, 0.0) for word in tf}\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    words = set(vec1.keys()).union(set(vec2.keys()))\n",
    "    v1 = np.array([vec1.get(w, 0.0) for w in words])\n",
    "    v2 = np.array([vec2.get(w, 0.0) for w in words])\n",
    "    dot = np.dot(v1, v2)\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    return dot / (norm1 * norm2) if norm1 and norm2 else 0.0\n",
    "\n",
    "def generate_distractors(answer, context, idf):\n",
    "    answer_tokens = preprocess(answer)\n",
    "    context_tokens = preprocess(context)\n",
    "    answer_vec = compute_tf_idf(compute_tf(answer_tokens), idf)\n",
    "\n",
    "    distractors = []\n",
    "    used_words = set([t.lower() for t in answer_tokens])\n",
    "    for word in set(context_tokens):\n",
    "        if word in used_words:\n",
    "            continue\n",
    "        if word in answer.lower() or answer.lower() in word:\n",
    "            continue\n",
    "        word_vec = compute_tf_idf(compute_tf([word]), idf)\n",
    "        sim = cosine_similarity(answer_vec, word_vec)\n",
    "        if 0.2 < sim < 0.8:\n",
    "            distractors.append((word, sim))\n",
    "\n",
    "    distractors = sorted(distractors, key=lambda x: -x[1])[:3]\n",
    "    return [w for w, _ in distractors]\n",
    "\n",
    "def get_relevant_sentence(text, answer):\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if answer.lower() in sent.lower():\n",
    "            return sent\n",
    "    return text\n",
    "\n",
    "def generate_question_squad_style(context, answer):\n",
    "    sentence = get_relevant_sentence(context, answer)\n",
    "    # Highlight answer with <hl> tags (case-insensitive)\n",
    "    pattern = re.compile(re.escape(answer), re.IGNORECASE)\n",
    "    highlighted_sentence = pattern.sub(f\"<hl>{answer}</hl>\", sentence, count=1)\n",
    "    input_text = f\"generate question: {highlighted_sentence} answer: {answer}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_ids = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
    "    question = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return question\n",
    "\n",
    "def generate_mcqs_from_text(text, desired_mcq_count=5):\n",
    "    candidate_answers = extract_named_entities(text)\n",
    "\n",
    "    # Fallback: if no named entities found, use frequent keywords\n",
    "    if not candidate_answers:\n",
    "        candidate_answers = preprocess(text)[:20]\n",
    "\n",
    "    idf = {word: 1.0 for word in preprocess(text)}  # dummy IDF for single doc\n",
    "\n",
    "    mcqs = []\n",
    "    used_answers = set()\n",
    "\n",
    "    for answer in candidate_answers:\n",
    "        if len(mcqs) >= desired_mcq_count:\n",
    "            break\n",
    "        if len(answer.split()) > 5 or len(answer) < 2:\n",
    "            continue\n",
    "        if answer.lower() in used_answers:\n",
    "            continue\n",
    "\n",
    "        distractors = generate_distractors(answer, text, idf)\n",
    "\n",
    "        # If less than 3 distractors found, add random words from context\n",
    "        if len(distractors) < 3:\n",
    "            context_tokens = set(preprocess(text))\n",
    "            random_distractors = list(context_tokens - set([answer.lower()]))\n",
    "            random.shuffle(random_distractors)\n",
    "            for w in random_distractors:\n",
    "                if w not in distractors and w.lower() != answer.lower():\n",
    "                    distractors.append(w)\n",
    "                if len(distractors) >= 3:\n",
    "                    break\n",
    "\n",
    "        if len(distractors) < 3:\n",
    "            continue\n",
    "\n",
    "        question = generate_question_squad_style(text, answer)\n",
    "        options = distractors[:3] + [answer]\n",
    "        random.shuffle(options)\n",
    "\n",
    "        mcqs.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "        used_answers.add(answer.lower())\n",
    "\n",
    "    return mcqs\n",
    "\n",
    "\n",
    "# ======== Example usage ========\n",
    "\n",
    "input_text = \"\"\"\n",
    "In the heart of the quaint village of Eldermere, a mysterious tree stood tall in the town square.\n",
    "Its gnarled branches bore fruits that resembled pears, but with an unusual twist: they seemed to\n",
    "shimmer with a golden hue. The villagers affectionately named it the \"Shakespear\" tree, believing it\n",
    "held magical properties.\n",
    "\n",
    "Legend had it that anyone who tasted a Shakespear would gain a glimpse into their future.\n",
    "Curiosity spread like wildfire, and soon, villagers flocked to the tree, eager for a taste of destiny.\n",
    "Young Emma, a spirited girl with dreams of becoming a writer, felt an undeniable pull toward the\n",
    "shimmering fruit.\n",
    "\n",
    "One crisp autumn morning, she approached the tree, heart racing. With a deep breath, she plucked a\n",
    "Shakespear and took a bite. Instantly, a whirlwind of visions enveloped her. She saw herself standing\n",
    "on a grand stage, the applause of a thousand voices echoing in her ears. In another glimpse, she wandered\n",
    "through enchanted forests, her stories coming to life.\n",
    "\n",
    "Determined to fulfill these dreams, Emma spent every spare moment writing. The villagers, inspired by\n",
    "her passion, began sharing their own tales. The square buzzed with creativity, and soon, Eldermere\n",
    "became a hub of storytelling.\n",
    "\n",
    "As the seasons changed, Emmaâ€™s words took flight. She published her first book, a collection of\n",
    "enchanting stories, and it captured the hearts of many beyond Eldermere. The Shakespear tree\n",
    "continued to stand, its golden pears glimmering, a reminder that dreams, when nurtured, could blossom\n",
    "into reality.\n",
    "\n",
    "And so, in the embrace of magic and creativity, the legacy of the Shakespear lived on, inspiring\n",
    "generations to reach for their dreams.\n",
    "\"\"\"\n",
    "\n",
    "mcqs = generate_mcqs_from_text(input_text, desired_mcq_count=3)\n",
    "\n",
    "for i, mcq in enumerate(mcqs):\n",
    "    print(f\"\\nQuestion {i+1}: {mcq['question']}\")\n",
    "    for j, opt in enumerate(mcq['options']):\n",
    "        print(f\"  {chr(65+j)}. {opt}\")\n",
    "    print(f\"Answer: {mcq['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e48c5c-a067-4472-94cf-0d6974f64a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfd2d9-f433-4f35-a714-0f7697db3356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647bd012-dd7e-4107-a9e7-51edb6e401c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e35f89-2705-4684-9c33-875b4d3a5e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ac9f914-2e5d-48ab-8d9b-8f6e284fa6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3a5e9-2e0a-48b5-b497-c85f62112fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d3c5b7c-3b1c-487a-9487-f7e0dcadb265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: Who felt an undeniable pull toward the shimmering fruit?\n",
      "  A. lived\n",
      "  B. captured\n",
      "  C. Young Emma\n",
      "  D. writing\n",
      "Answer: Young Emma\n",
      "\n",
      "Question 2: What village is Eldermere located in?\n",
      "  A. spread\n",
      "  B. glimpse\n",
      "  C. published\n",
      "  D. Eldermere\n",
      "Answer: Eldermere\n",
      "\n",
      "Question 3: What was the name of the tree that stood tall in the town square?\n",
      "  A. Shakespear Enigma\n",
      "  B. taste\n",
      "  C. coming\n",
      "  D. unusual\n",
      "Answer: Shakespear Enigma\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import fitz  # PyMuPDF for PDF reading\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk, sent_tokenize\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Download required NLTK data (only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the fine-tuned T5 model and tokenizer for SQuAD-style QG\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "# ðŸ“„ Utility to read PDF and return extracted text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    tree = ne_chunk(tagged, binary=False)\n",
    "    entities = set()\n",
    "\n",
    "    for subtree in tree:\n",
    "        if hasattr(subtree, 'label') and subtree.label() in ['PERSON', 'ORGANIZATION', 'GPE', 'LOCATION', 'FACILITY']:\n",
    "            entity = ' '.join([token for token, pos in subtree.leaves()])\n",
    "            if len(entity) > 1:\n",
    "                entities.add(entity)\n",
    "    return list(entities)\n",
    "\n",
    "def compute_tf(tokens):\n",
    "    tf = {}\n",
    "    for word in tokens:\n",
    "        tf[word] = tf.get(word, 0) + 1\n",
    "    total = len(tokens)\n",
    "    for word in tf:\n",
    "        tf[word] /= total\n",
    "    return tf\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    return {word: tf[word] * idf.get(word, 0.0) for word in tf}\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    words = set(vec1.keys()).union(set(vec2.keys()))\n",
    "    v1 = np.array([vec1.get(w, 0.0) for w in words])\n",
    "    v2 = np.array([vec2.get(w, 0.0) for w in words])\n",
    "    dot = np.dot(v1, v2)\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    return dot / (norm1 * norm2) if norm1 and norm2 else 0.0\n",
    "\n",
    "def generate_distractors(answer, context, idf):\n",
    "    answer_tokens = preprocess(answer)\n",
    "    context_tokens = preprocess(context)\n",
    "    answer_vec = compute_tf_idf(compute_tf(answer_tokens), idf)\n",
    "\n",
    "    distractors = []\n",
    "    used_words = set([t.lower() for t in answer_tokens])\n",
    "    for word in set(context_tokens):\n",
    "        if word in used_words:\n",
    "            continue\n",
    "        if word in answer.lower() or answer.lower() in word:\n",
    "            continue\n",
    "        word_vec = compute_tf_idf(compute_tf([word]), idf)\n",
    "        sim = cosine_similarity(answer_vec, word_vec)\n",
    "        if 0.2 < sim < 0.8:\n",
    "            distractors.append((word, sim))\n",
    "\n",
    "    distractors = sorted(distractors, key=lambda x: -x[1])[:3]\n",
    "    return [w for w, _ in distractors]\n",
    "\n",
    "def get_relevant_sentence(text, answer):\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if answer.lower() in sent.lower():\n",
    "            return sent\n",
    "    return text\n",
    "\n",
    "def generate_question_squad_style(context, answer):\n",
    "    sentence = get_relevant_sentence(context, answer)\n",
    "    pattern = re.compile(re.escape(answer), re.IGNORECASE)\n",
    "    highlighted_sentence = pattern.sub(f\"<hl>{answer}</hl>\", sentence, count=1)\n",
    "    input_text = f\"generate question: {highlighted_sentence} answer: {answer}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_ids = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
    "    question = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return question\n",
    "\n",
    "def generate_mcqs_from_text(text, desired_mcq_count=5):\n",
    "    candidate_answers = extract_named_entities(text)\n",
    "\n",
    "    if not candidate_answers:\n",
    "        candidate_answers = preprocess(text)[:20]\n",
    "\n",
    "    idf = {word: 1.0 for word in preprocess(text)}  # dummy IDF for single doc\n",
    "\n",
    "    mcqs = []\n",
    "    used_answers = set()\n",
    "\n",
    "    for answer in candidate_answers:\n",
    "        if len(mcqs) >= desired_mcq_count:\n",
    "            break\n",
    "        if len(answer.split()) > 5 or len(answer) < 2:\n",
    "            continue\n",
    "        if answer.lower() in used_answers:\n",
    "            continue\n",
    "\n",
    "        distractors = generate_distractors(answer, text, idf)\n",
    "\n",
    "        if len(distractors) < 3:\n",
    "            context_tokens = set(preprocess(text))\n",
    "            random_distractors = list(context_tokens - set([answer.lower()]))\n",
    "            random.shuffle(random_distractors)\n",
    "            for w in random_distractors:\n",
    "                if w not in distractors and w.lower() != answer.lower():\n",
    "                    distractors.append(w)\n",
    "                if len(distractors) >= 3:\n",
    "                    break\n",
    "\n",
    "        if len(distractors) < 3:\n",
    "            continue\n",
    "\n",
    "        question = generate_question_squad_style(text, answer)\n",
    "        options = distractors[:3] + [answer]\n",
    "        random.shuffle(options)\n",
    "\n",
    "        mcqs.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "        used_answers.add(answer.lower())\n",
    "\n",
    "    return mcqs\n",
    "\n",
    "# ======= MAIN EXECUTION =======\n",
    "\n",
    "def run_mcq_generator(input_source, desired_mcq_count=3):\n",
    "    if input_source.lower().endswith(\".pdf\"):\n",
    "        text = extract_text_from_pdf(input_source)\n",
    "    else:\n",
    "        text = input_source  # assume plain text\n",
    "\n",
    "    mcqs = generate_mcqs_from_text(text, desired_mcq_count)\n",
    "\n",
    "    for i, mcq in enumerate(mcqs):\n",
    "        print(f\"\\nQuestion {i+1}: {mcq['question']}\")\n",
    "        for j, opt in enumerate(mcq['options']):\n",
    "            print(f\"  {chr(65+j)}. {opt}\")\n",
    "        print(f\"Answer: {mcq['answer']}\")\n",
    "\n",
    "# Example usage for text:\n",
    "\n",
    "# input_text = \"\"\"\n",
    "# In the heart of the quaint village of Eldermere, a mysterious tree stood tall in the town square.\n",
    "# Its gnarled branches bore fruits that resembled pears, but with an unusual twist: they seemed to\n",
    "# shimmer with a golden hue. The villagers affectionately named it the \"Shakespear\" tree, believing it\n",
    "# held magical properties.\n",
    "\n",
    "# Legend had it that anyone who tasted a Shakespear would gain a glimpse into their future.\n",
    "# Curiosity spread like wildfire, and soon, villagers flocked to the tree, eager for a taste of destiny.\n",
    "# Young Emma, a spirited girl with dreams of becoming a writer, felt an undeniable pull toward the\n",
    "# shimmering fruit.\n",
    "\n",
    "# One crisp autumn morning, she approached the tree, heart racing. With a deep breath, she plucked a\n",
    "# Shakespear and took a bite. Instantly, a whirlwind of visions enveloped her. She saw herself standing\n",
    "# on a grand stage, the applause of a thousand voices echoing in her ears. In another glimpse, she wandered\n",
    "# through enchanted forests, her stories coming to life.\n",
    "\n",
    "# Determined to fulfill these dreams, Emma spent every spare moment writing. The villagers, inspired by\n",
    "# her passion, began sharing their own tales. The square buzzed with creativity, and soon, Eldermere\n",
    "# became a hub of storytelling.\n",
    "\n",
    "# As the seasons changed, Emmaâ€™s words took flight. She published her first book, a collection of\n",
    "# enchanting stories, and it captured the hearts of many beyond Eldermere. The Shakespear tree\n",
    "# continued to stand, its golden pears glimmering, a reminder that dreams, when nurtured, could blossom\n",
    "# into reality.\n",
    "\n",
    "# And so, in the embrace of magic and creativity, the legacy of the Shakespear lived on, inspiring\n",
    "# generations to reach for their dreams.\n",
    "# \"\"\"\n",
    "# run_mcq_generator(input_text)\n",
    "\n",
    "# Example usage for PDF:\n",
    "# C:\\Users\\atulm\\Desktop\n",
    "run_mcq_generator(r\"C:\\Users\\atulm\\Desktop\\sample.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d52f15-d630-4951-8941-8f230265f609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34694c57-6414-4c93-95af-1f4d5aa19fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfafee6-7967-44b7-bb1e-22a8b5065f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadb134-8316-47ac-afee-798058a9b0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd79ed-6295-4cd2-b4ba-3d07818fbf5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a63e6e-db10-4c4c-b6da-a3a0b8a5f68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778f9a3-d359-43d7-9972-7e29366653cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rule based also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af741aef-9793-4e9e-96cf-5d1ebc53bdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90053ac0-84e0-4fe1-8218-e340d20f57a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating MCQs from plain text...\n",
      "\n",
      "\n",
      "Question 1: What is the name of the village in which a mysterious tree stood in the square?\n",
      "  A. sharing\n",
      "  B. village\n",
      "  C. Eldermere\n",
      "  D. branch\n",
      "Answer: Eldermere\n",
      "\n",
      "Question 2: Who felt an undeniable pull toward the shimmering fruit?\n",
      "  A. racing\n",
      "  B. took\n",
      "  C. unusual\n",
      "  D. Young Emma\n",
      "Answer: Young Emma\n",
      "\n",
      "Question 3: What did the villagers call the tree?\n",
      "  A. Shakespear\n",
      "  B. bite\n",
      "  C. changed\n",
      "  D. writer\n",
      "Answer: Shakespear\n",
      "\n",
      "Question 4: What mysterious object stood tall in the town square?\n",
      "  A. coming\n",
      "  B. tree\n",
      "  C. village\n",
      "  D. first\n",
      "Answer: tree\n",
      "\n",
      "Question 5: What is the name of the tree that stood tall in the town square?\n",
      "  A. village\n",
      "  B. heart\n",
      "  C. held\n",
      "  D. glimmering\n",
      "Answer: heart\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import fitz  # PyMuPDF for PDF reading\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk, sent_tokenize\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Setup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "def read_pdf_text(file_path):\n",
    "    \"\"\"Extracts all text from a PDF file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {file_path}\")\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    tree = ne_chunk(tagged, binary=False)\n",
    "    entities = set()\n",
    "    for subtree in tree:\n",
    "        if hasattr(subtree, 'label') and subtree.label() in ['PERSON', 'ORGANIZATION', 'GPE', 'LOCATION', 'FACILITY']:\n",
    "            entity = ' '.join([token for token, pos in subtree.leaves()])\n",
    "            if len(entity) > 1:\n",
    "                entities.add(entity)\n",
    "    return list(entities)\n",
    "\n",
    "def compute_tf(tokens):\n",
    "    tf = {}\n",
    "    for word in tokens:\n",
    "        tf[word] = tf.get(word, 0) + 1\n",
    "    total = len(tokens)\n",
    "    for word in tf:\n",
    "        tf[word] /= total\n",
    "    return tf\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    return {word: tf[word] * idf.get(word, 0.0) for word in tf}\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    words = set(vec1.keys()).union(set(vec2.keys()))\n",
    "    v1 = np.array([vec1.get(w, 0.0) for w in words])\n",
    "    v2 = np.array([vec2.get(w, 0.0) for w in words])\n",
    "    dot = np.dot(v1, v2)\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    return dot / (norm1 * norm2) if norm1 and norm2 else 0.0\n",
    "\n",
    "def generate_distractors(answer, context, idf, required=3):\n",
    "    answer_tokens = preprocess(answer)\n",
    "    context_tokens = preprocess(context)\n",
    "    answer_vec = compute_tf_idf(compute_tf(answer_tokens), idf)\n",
    "    distractors = []\n",
    "    used_words = set([t.lower() for t in answer_tokens])\n",
    "\n",
    "    for word in set(context_tokens):\n",
    "        if word in used_words or word in answer.lower() or answer.lower() in word:\n",
    "            continue\n",
    "        word_vec = compute_tf_idf(compute_tf([word]), idf)\n",
    "        sim = cosine_similarity(answer_vec, word_vec)\n",
    "        if 0.1 < sim < 0.9:\n",
    "            distractors.append((word, sim))\n",
    "\n",
    "    distractors = sorted(distractors, key=lambda x: -x[1])\n",
    "    distractor_words = [w for w, _ in distractors]\n",
    "\n",
    "    if len(distractor_words) < required:\n",
    "        extras = list(set(context_tokens) - set(distractor_words) - set(answer_tokens))\n",
    "        random.shuffle(extras)\n",
    "        distractor_words += extras[:required - len(distractor_words)]\n",
    "\n",
    "    return distractor_words[:required]\n",
    "\n",
    "def get_relevant_sentence(text, answer):\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if answer.lower() in sent.lower():\n",
    "            return sent\n",
    "    return text\n",
    "\n",
    "def generate_question_rule_based(sentence, answer):\n",
    "    if answer.istitle():\n",
    "        return f\"Who is {answer}?\"\n",
    "    elif any(word in answer.lower() for word in ['village', 'city', 'place', 'school', 'forest']):\n",
    "        return f\"Where is {answer} located?\"\n",
    "    else:\n",
    "        return f\"What is {answer}?\"\n",
    "\n",
    "def generate_question_with_fallback(context, answer):\n",
    "    sentence = get_relevant_sentence(context, answer)\n",
    "    pattern = re.compile(re.escape(answer), re.IGNORECASE)\n",
    "    highlighted_sentence = pattern.sub(f\"<hl>{answer}</hl>\", sentence, count=1)\n",
    "    input_text = f\"generate question: {highlighted_sentence} answer: {answer}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_ids = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
    "    question = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    if not question or answer.lower() in question.lower():\n",
    "        question = generate_question_rule_based(sentence, answer)\n",
    "        print(f\"Rule-based fallback triggered for answer: '{answer}'\")\n",
    "    return question\n",
    "\n",
    "def is_similar(q1, q2, threshold=0.85):\n",
    "    return SequenceMatcher(None, q1.lower(), q2.lower()).ratio() >= threshold\n",
    "\n",
    "def generate_mcqs_from_text(text, desired_mcq_count=5):\n",
    "    candidate_answers = extract_named_entities(text)\n",
    "    if len(candidate_answers) < desired_mcq_count:\n",
    "        freq_words = Counter(preprocess(text)).most_common(50)\n",
    "        for word, _ in freq_words:\n",
    "            if word not in candidate_answers and word.isalpha() and len(word) > 2:\n",
    "                candidate_answers.append(word)\n",
    "\n",
    "    idf = {word: 1.0 for word in preprocess(text)}\n",
    "    mcqs = []\n",
    "    used_questions = []\n",
    "\n",
    "    for answer in candidate_answers:\n",
    "        if len(mcqs) >= desired_mcq_count:\n",
    "            break\n",
    "        if len(answer.split()) > 5 or len(answer) < 2:\n",
    "            continue\n",
    "\n",
    "        distractors = generate_distractors(answer, text, idf, required=3)\n",
    "        if len(set(distractors)) < 3:\n",
    "            continue\n",
    "\n",
    "        question = generate_question_with_fallback(text, answer)\n",
    "        if any(is_similar(question, existing_q) for existing_q in used_questions):\n",
    "            continue\n",
    "\n",
    "        options = distractors[:3] + [answer]\n",
    "        random.shuffle(options)\n",
    "\n",
    "        mcqs.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "        used_questions.append(question)\n",
    "\n",
    "    return mcqs\n",
    "\n",
    "# -------- Switch here --------\n",
    "use_pdf = False  # Set to True to read from PDF, False for plain text\n",
    "\n",
    "if use_pdf:\n",
    "    pdf_path = \"your_pdf_file.pdf\"  # <-- Set your PDF file path here\n",
    "    input_text = read_pdf_text(pdf_path)\n",
    "else:\n",
    "    input_text = \"\"\"\n",
    "    In the heart of the quaint village of Eldermere, a mysterious tree stood tall in the town square.\n",
    "    Its gnarled branches bore fruits that resembled pears, but with an unusual twist: they seemed to\n",
    "    shimmer with a golden hue. The villagers affectionately named it the \"Shakespear\" tree, believing it\n",
    "    held magical properties.\n",
    "\n",
    "    Legend had it that anyone who tasted a Shakespear would gain a glimpse into their future.\n",
    "    Curiosity spread like wildfire, and soon, villagers flocked to the tree, eager for a taste of destiny.\n",
    "    Young Emma, a spirited girl with dreams of becoming a writer, felt an undeniable pull toward the\n",
    "    shimmering fruit.\n",
    "\n",
    "    One crisp autumn morning, she approached the tree, heart racing. With a deep breath, she plucked a\n",
    "    Shakespear and took a bite. Instantly, a whirlwind of visions enveloped her. She saw herself standing\n",
    "    on a grand stage, the applause of a thousand voices echoing in her ears. In another glimpse, she wandered\n",
    "    through enchanted forests, her stories coming to life.\n",
    "\n",
    "    Determined to fulfill these dreams, Emma spent every spare moment writing. The villagers, inspired by\n",
    "    her passion, began sharing their own tales. The square buzzed with creativity, and soon, Eldermere\n",
    "    became a hub of storytelling.\n",
    "\n",
    "    As the seasons changed, Emmaâ€™s words took flight. She published her first book, a collection of\n",
    "    enchanting stories, and it captured the hearts of many beyond Eldermere. The Shakespear tree\n",
    "    continued to stand, its golden pears glimmering, a reminder that dreams, when nurtured, could blossom\n",
    "    into reality.\n",
    "    \"\"\"\n",
    "\n",
    "print(f\"\\nGenerating MCQs from {'PDF file' if use_pdf else 'plain text'}...\\n\")\n",
    "mcqs = generate_mcqs_from_text(input_text, desired_mcq_count=5)\n",
    "\n",
    "for i, mcq in enumerate(mcqs):\n",
    "    print(f\"\\nQuestion {i+1}: {mcq['question']}\")\n",
    "    for j, opt in enumerate(mcq['options']):\n",
    "        print(f\"  {chr(65+j)}. {opt}\")\n",
    "    print(f\"Answer: {mcq['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443a3ae-6ee3-4488-94e6-a627d9dfe423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f58145-d1a0-40d7-a68f-ec1525c21009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b09ed9-ae48-4dd8-856b-855b357502d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33a16e-0127-49e3-873a-c2333bfaed63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181dfa0-ae92-4925-b2b0-175799bfccef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be57189-2fe3-4048-8bcb-6dae0a57b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if t5 fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c5bf29c-26ad-4254-b6e5-dfea98376f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based fallback triggered for answer: 'Shakespear'\n",
      "Rule-based fallback triggered for answer: 'shakespear'\n",
      "\n",
      "Question 1: What is the name of the village in which a mysterious tree stood in the square?\n",
      "  A. Eldermere\n",
      "  B. named\n",
      "  C. heart\n",
      "  D. fruit\n",
      "Answer: Eldermere\n",
      "\n",
      "Question 2: Who felt an undeniable pull toward the shimmering fruit?\n",
      "  A. morning\n",
      "  B. instantly\n",
      "  C. Young Emma\n",
      "  D. legend\n",
      "Answer: Young Emma\n",
      "\n",
      "Question 3: Who is Shakespear?\n",
      "  A. spread\n",
      "  B. Shakespear\n",
      "  C. felt\n",
      "  D. spent\n",
      "Answer: Shakespear\n",
      "\n",
      "Question 4: What mysterious object stood tall in the town square?\n",
      "  A. fulfill\n",
      "  B. tree\n",
      "  C. inspired\n",
      "  D. tale\n",
      "Answer: tree\n",
      "\n",
      "Question 5: What is the name of the tree that stood tall in the town square?\n",
      "  A. wildfire\n",
      "  B. shakespear\n",
      "  C. heart\n",
      "  D. resembled\n",
      "Answer: heart\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk, sent_tokenize\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Setup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"valhalla/t5-base-qg-hl\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    tree = ne_chunk(tagged, binary=False)\n",
    "    entities = set()\n",
    "    for subtree in tree:\n",
    "        if hasattr(subtree, 'label') and subtree.label() in ['PERSON', 'ORGANIZATION', 'GPE', 'LOCATION', 'FACILITY']:\n",
    "            entity = ' '.join([token for token, pos in subtree.leaves()])\n",
    "            if len(entity) > 1:\n",
    "                entities.add(entity)\n",
    "    return list(entities)\n",
    "\n",
    "def compute_tf(tokens):\n",
    "    tf = {}\n",
    "    for word in tokens:\n",
    "        tf[word] = tf.get(word, 0) + 1\n",
    "    total = len(tokens)\n",
    "    for word in tf:\n",
    "        tf[word] /= total\n",
    "    return tf\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    return {word: tf[word] * idf.get(word, 0.0) for word in tf}\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    words = set(vec1.keys()).union(set(vec2.keys()))\n",
    "    v1 = np.array([vec1.get(w, 0.0) for w in words])\n",
    "    v2 = np.array([vec2.get(w, 0.0) for w in words])\n",
    "    dot = np.dot(v1, v2)\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    return dot / (norm1 * norm2) if norm1 and norm2 else 0.0\n",
    "\n",
    "def generate_distractors(answer, context, idf, required=3):\n",
    "    answer_tokens = preprocess(answer)\n",
    "    context_tokens = preprocess(context)\n",
    "    answer_vec = compute_tf_idf(compute_tf(answer_tokens), idf)\n",
    "    distractors = []\n",
    "    used_words = set([t.lower() for t in answer_tokens])\n",
    "\n",
    "    for word in set(context_tokens):\n",
    "        if word in used_words or word in answer.lower() or answer.lower() in word:\n",
    "            continue\n",
    "        word_vec = compute_tf_idf(compute_tf([word]), idf)\n",
    "        sim = cosine_similarity(answer_vec, word_vec)\n",
    "        if 0.1 < sim < 0.9:\n",
    "            distractors.append((word, sim))\n",
    "\n",
    "    distractors = sorted(distractors, key=lambda x: -x[1])\n",
    "    distractor_words = [w for w, _ in distractors]\n",
    "\n",
    "    if len(distractor_words) < required:\n",
    "        extras = list(set(context_tokens) - set(distractor_words) - set(answer_tokens))\n",
    "        random.shuffle(extras)\n",
    "        distractor_words += extras[:required - len(distractor_words)]\n",
    "\n",
    "    return distractor_words[:required]\n",
    "\n",
    "def get_relevant_sentence(text, answer):\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if answer.lower() in sent.lower():\n",
    "            return sent\n",
    "    return text\n",
    "\n",
    "def generate_question_rule_based(sentence, answer):\n",
    "    if answer.istitle():\n",
    "        return f\"Who is {answer}?\"\n",
    "    elif any(word in answer.lower() for word in ['village', 'city', 'place', 'school', 'forest']):\n",
    "        return f\"Where is {answer} located?\"\n",
    "    else:\n",
    "        return f\"What is {answer}?\"\n",
    "\n",
    "def generate_question_with_fallback(context, answer):\n",
    "    sentence = get_relevant_sentence(context, answer)\n",
    "    pattern = re.compile(re.escape(answer), re.IGNORECASE)\n",
    "    highlighted_sentence = pattern.sub(f\"<hl>{answer}</hl>\", sentence, count=1)\n",
    "    input_text = f\"generate question: {highlighted_sentence} answer: {answer}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_ids = model.generate(input_ids, max_length=64, num_beams=4, early_stopping=True)\n",
    "    question = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # === Simulate failure of T5 for demonstration ===\n",
    "    if answer.lower() == \"shakespear\":\n",
    "        question = \"\"  # Force empty output to trigger fallback\n",
    "\n",
    "    if not question or answer.lower() in question.lower():\n",
    "        question = generate_question_rule_based(sentence, answer)\n",
    "        print(f\"Rule-based fallback triggered for answer: '{answer}'\")  # Debug output\n",
    "\n",
    "    return question\n",
    "\n",
    "def is_similar(q1, q2, threshold=0.85):\n",
    "    return SequenceMatcher(None, q1.lower(), q2.lower()).ratio() >= threshold\n",
    "\n",
    "def generate_mcqs_from_text(text, desired_mcq_count=5):\n",
    "    candidate_answers = extract_named_entities(text)\n",
    "    if len(candidate_answers) < desired_mcq_count:\n",
    "        freq_words = Counter(preprocess(text)).most_common(50)\n",
    "        for word, _ in freq_words:\n",
    "            if word not in candidate_answers and word.isalpha() and len(word) > 2:\n",
    "                candidate_answers.append(word)\n",
    "\n",
    "    idf = {word: 1.0 for word in preprocess(text)}\n",
    "    mcqs = []\n",
    "    used_questions = []\n",
    "\n",
    "    for answer in candidate_answers:\n",
    "        if len(mcqs) >= desired_mcq_count:\n",
    "            break\n",
    "        if len(answer.split()) > 5 or len(answer) < 2:\n",
    "            continue\n",
    "\n",
    "        distractors = generate_distractors(answer, text, idf, required=3)\n",
    "        if len(set(distractors)) < 3:\n",
    "            continue\n",
    "\n",
    "        question = generate_question_with_fallback(text, answer)\n",
    "        if any(is_similar(question, existing_q) for existing_q in used_questions):\n",
    "            continue\n",
    "\n",
    "        options = distractors[:3] + [answer]\n",
    "        random.shuffle(options)\n",
    "\n",
    "        mcqs.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "        used_questions.append(question)\n",
    "\n",
    "    return mcqs\n",
    "\n",
    "# ======= SAMPLE TEXT =======\n",
    "input_text = \"\"\"\n",
    "In the heart of the quaint village of Eldermere, a mysterious tree stood tall in the town square.\n",
    "Its gnarled branches bore fruits that resembled pears, but with an unusual twist: they seemed to\n",
    "shimmer with a golden hue. The villagers affectionately named it the \"Shakespear\" tree, believing it\n",
    "held magical properties.\n",
    "\n",
    "Legend had it that anyone who tasted a Shakespear would gain a glimpse into their future.\n",
    "Curiosity spread like wildfire, and soon, villagers flocked to the tree, eager for a taste of destiny.\n",
    "Young Emma, a spirited girl with dreams of becoming a writer, felt an undeniable pull toward the\n",
    "shimmering fruit.\n",
    "\n",
    "One crisp autumn morning, she approached the tree, heart racing. With a deep breath, she plucked a\n",
    "Shakespear and took a bite. Instantly, a whirlwind of visions enveloped her. She saw herself standing\n",
    "on a grand stage, the applause of a thousand voices echoing in her ears. In another glimpse, she wandered\n",
    "through enchanted forests, her stories coming to life.\n",
    "\n",
    "Determined to fulfill these dreams, Emma spent every spare moment writing. The villagers, inspired by\n",
    "her passion, began sharing their own tales. The square buzzed with creativity, and soon, Eldermere\n",
    "became a hub of storytelling.\n",
    "\n",
    "As the seasons changed, Emmaâ€™s words took flight. She published her first book, a collection of\n",
    "enchanting stories, and it captured the hearts of many beyond Eldermere. The Shakespear tree\n",
    "continued to stand, its golden pears glimmering, a reminder that dreams, when nurtured, could blossom\n",
    "into reality.\n",
    "\"\"\"\n",
    "\n",
    "# ========== RUN ==========\n",
    "mcqs = generate_mcqs_from_text(input_text, desired_mcq_count=5)\n",
    "for i, mcq in enumerate(mcqs):\n",
    "    print(f\"\\nQuestion {i+1}: {mcq['question']}\")\n",
    "    for j, opt in enumerate(mcq['options']):\n",
    "        print(f\"  {chr(65+j)}. {opt}\")\n",
    "    print(f\"Answer: {mcq['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1519136-ab9b-420e-9fb1-762fe0c6c772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
