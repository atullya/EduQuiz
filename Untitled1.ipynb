{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620d38a-11a2-485e-b8d3-552ff4ae02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab8aa3e-76d0-4a41-8e13-5132c3dea1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464653a0-1154-4665-bc73-99edf4d57b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: yake in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.4.8)\n",
      "Requirement already satisfied: tabulate in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from yake) (0.9.0)\n",
      "Requirement already satisfied: click>=6.0 in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from yake) (8.1.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from yake) (2.2.6)\n",
      "Requirement already satisfied: segtok in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from yake) (1.5.11)\n",
      "Requirement already satisfied: networkx in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from yake) (3.4.2)\n",
      "Requirement already satisfied: jellyfish in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from yake) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click>=6.0->yake) (0.4.6)\n",
      "Requirement already satisfied: regex in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from segtok->yake) (2024.11.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d90358aa-2cc9-46a5-91e1-7f2ee69446c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eldermere', 'square', 'tree', 'Shakespear', 'heart', 'quaint', 'village', 'mysterious', 'stood', 'tall', 'town', 'villagers', 'Emma', 'pears', 'twist', 'hue', 'gnarled', 'branches', 'bore', 'resembled', 'unusual', 'shimmer', 'golden', 'believing', 'properties']\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "def getImportantWords(art, top_n=25):\n",
    "    # Create a YAKE keyword extractor\n",
    "    kw_extractor = yake.KeywordExtractor(lan=\"en\", n=1, top=top_n)  \n",
    "    # lan=\"en\" = English language\n",
    "    # n=1 = extract unigrams (single-word keywords, similar to PROPN in pke)\n",
    "\n",
    "    # Extract keywords\n",
    "    keywords = kw_extractor.extract_keywords(art)\n",
    "\n",
    "    # Get just the words, not the scores\n",
    "    result = [kw for kw, score in keywords]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "text = \"\"\"\n",
    "In the heart of the quaint village of Eldermere, a mysterious tree stood tall in the town square. \\nIts gnarled branches bore fruits that resembled pears, but with an unusual twist: they seemed to shimmer with a golden hue. \\nThe villagers affectionately named it the 'Shakespear' tree, believing it held magical properties.\\nLegend had it that anyone who tasted a Shakespear would gain a glimpse into their future. Curiosity\\nspread like wildfire, and soon, villagers flocked to the tree, eager for a taste of destiny. Young Emma, a\\nspirited girl with dreams of becoming a writer, felt an undeniable pull toward the shimmering fruit.\n",
    "\"\"\"\n",
    "impWords = getImportantWords(text)\n",
    "print(impWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a8f43ad-9839-4f8d-a022-d191eb166b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3- Split the whole text article into an array/list of individual sentences. This will help us fetch the sentences related to the keywords easily\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def splitTextToSents(art):\n",
    "    s=[sent_tokenize(art)]\n",
    "    s=[y for x in s for y in x]\n",
    "    s=[sent.strip() for sent in s if len(sent)>15] #Removes all the sentences that have length less than 15 so that we can ensure that our questions have enough length for context\n",
    "    return s\n",
    "sents=splitTextToSents(text) #Achieve a well splitted set of sentences from the text article\n",
    "#print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ded693c-f681-4db5-aa84-8c82095874cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the heart of the quaint village of Eldermere, a mysterious tree stood tall in the town square.',\n",
       " 'Its gnarled branches bore fruits that resembled pears, but with an unusual twist: they seemed to shimmer with a golden hue.',\n",
       " \"The villagers affectionately named it the 'Shakespear' tree, believing it held magical properties.\",\n",
       " 'Legend had it that anyone who tasted a Shakespear would gain a glimpse into their future.',\n",
       " 'Curiosity\\nspread like wildfire, and soon, villagers flocked to the tree, eager for a taste of destiny.',\n",
       " 'Young Emma, a\\nspirited girl with dreams of becoming a writer, felt an undeniable pull toward the shimmering fruit.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f0461e-d0bd-4dcd-a04d-9a2e2a3f5664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: flashtext in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.7)\n"
     ]
    }
   ],
   "source": [
    "#Step 4- Map the sentences which contain the keywords to the related keywords so that we can easily lookup the sentences related to the keywords\n",
    "!pip install flashtext\n",
    "from flashtext import KeywordProcessor\n",
    "def mapSents(impWords,sents):\n",
    "    processor=KeywordProcessor() #Using keyword processor as our processor for this task\n",
    "    keySents={}\n",
    "    for word in impWords:\n",
    "        keySents[word]=[]\n",
    "        processor.add_keyword(word) #Adds key word to the processor\n",
    "    for sent in sents:\n",
    "        found=processor.extract_keywords(sent) #Extract the keywords in the sentence\n",
    "        for each in found:\n",
    "            keySents[each].append(sent) #For each keyword found, map the sentence to the keyword\n",
    "    for key in keySents.keys():\n",
    "        temp=keySents[key]\n",
    "        temp=sorted(temp,key=len,reverse=True) #Sort the sentences according to their decreasing length in order to ensure the quality of question for the MCQ \n",
    "        keySents[key]=temp\n",
    "    return keySents\n",
    "mappedSents=mapSents(impWords,sents) #Achieve the sentences that contain the keywords and map those sentences to the keywords using this function\n",
    "#print(mappedSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c91efef-245a-40d0-ad90-b77a8d0b75e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 17.83272933959961 secs.\n"
     ]
    }
   ],
   "source": [
    "#Step 5- Get the sense of the word. In order to attain a quality set of distractors we need to get the right sense of the keyword. This is explained in detail in the seperate alogrithm documentation\n",
    "\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "def getWordSense(sent,word):\n",
    "    word=word.lower() \n",
    "    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n",
    "        word=word.replace(\" \",\"_\")\n",
    "    synsets=wn.synsets(word,'n') #Sysnets from Google are invoked\n",
    "    if synsets:\n",
    "        wup=max_similarity(sent,word,'wup',pos='n')\n",
    "        adapted_lesk_output = adapted_lesk(sent, word, pos='n')\n",
    "        lowest_index=min(synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "        return synsets[lowest_index]\n",
    "    else:\n",
    "        return None\n",
    "#print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c803fb6-c4e4-49b4-9013-d6856727adfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882a9b8-99a9-4f71-9aa2-231ae0dd5c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2da148cf-627b-4dbc-9fa5-1f51249b6468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rake-nltk\n",
      "  Downloading rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rake-nltk) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\atulm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk<4.0.0,>=3.6.2->rake-nltk) (0.4.6)\n",
      "Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Installing collected packages: rake-nltk\n",
      "Successfully installed rake-nltk-1.0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\atulm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading rake_nltk: Package 'rake_nltk' not found in\n",
      "[nltk_data]     index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is special about Emma in the story?\n",
      "A. Legend\n",
      "B. Magic\n",
      "C. Emma\n",
      "D. Tree\n",
      "Answer: Emma\n",
      "\n",
      "Q: What is special about One in the story?\n",
      "A. Duodecimal digit\n",
      "B. One\n",
      "C. Septet\n",
      "D. 2\n",
      "Answer: One\n",
      "\n",
      "Q: What is special about Gnarled in the story?\n",
      "A. Legend\n",
      "B. Tree\n",
      "C. Gnarled\n",
      "D. Magic\n",
      "Answer: Gnarled\n",
      "\n",
      "Q: What is special about Curiosity in the story?\n",
      "A. Muddiness\n",
      "B. Curiosity\n",
      "C. Inwardness\n",
      "D. Consciousness\n",
      "Answer: Curiosity\n",
      "\n",
      "Q: What is special about Mysterious in the story?\n",
      "A. Mysterious\n",
      "B. Tree\n",
      "C. Magic\n",
      "D. Legend\n",
      "Answer: Mysterious\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install rake-nltk\n",
    "import re\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('rake_nltk')\n",
    "\n",
    "# Input text\n",
    "text = \"\"\"\n",
    "In the heart of the quaint village of Eldermere, a mysterious tree stood tall in the town square. Its gnarled branches bore fruits that resembled pears, but with an unusual twist: they seemed to shimmer with a golden hue. The villagers affectionately named it the 'Shakespear' tree, believing it held magical properties.\n",
    "Legend had it that anyone who tasted a Shakespear would gain a glimpse into their future. Curiosity\n",
    "spread like wildfire, and soon, villagers flocked to the tree, eager for a taste of destiny. Young Emma, a\n",
    "spirited girl with dreams of becoming a writer, felt an undeniable pull toward the shimmering fruit.\n",
    "One crisp autumn morning, she approached the tree, heart racing. With a deep breath, she plucked a\n",
    "Shakespear and took a bite. Instantly, a whirlwind of visions enveloped her. She saw herself standing on a\n",
    "grand stage, the applause of a thousand voices echoing in her ears. In another glimpse, she wandered\n",
    "through enchanted forests, her stories coming to life.\n",
    "Determined to fulfill these dreams, Emma spent every spare moment writing. The villagers, inspired by\n",
    "her passion, began sharing their own tales. The square buzzed with creativity, and soon, Eldermere\n",
    "became a hub of storytelling.\n",
    "As the seasons changed, Emma’s words took flight. She published her first book, a collection of\n",
    "enchanting stories, and it captured the hearts of many beyond Eldermere. The Shakespear tree\n",
    "continued to stand, its golden pears glimmering, a reminder that dreams, when nurtured, could blossom\n",
    "into reality.\n",
    "And so, in the embrace of magic and creativity, the legacy of the Shakespear lived on, inspiring\n",
    "generations to reach for their dreams.\n",
    "\"\"\"\n",
    "\n",
    "# Extract keywords\n",
    "def extract_keywords(text, top_n=5):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    phrases = r.get_ranked_phrases()\n",
    "    return phrases[:top_n]\n",
    "\n",
    "# Generate distractors using WordNet\n",
    "def generate_distractors(word):\n",
    "    distractors = set()\n",
    "    synsets = wn.synsets(word, pos=wn.NOUN)\n",
    "    if synsets:\n",
    "        syn = synsets[0]\n",
    "        hypernyms = syn.hypernyms()\n",
    "        for hyper in hypernyms:\n",
    "            for hyponym in hyper.hyponyms():\n",
    "                for lemma in hyponym.lemmas():\n",
    "                    name = lemma.name().replace('_', ' ').capitalize()\n",
    "                    if name.lower() != word.lower():\n",
    "                        distractors.add(name)\n",
    "    return list(distractors)[:3]\n",
    "\n",
    "# Create an MCQ\n",
    "def create_mcq(keyword, text):\n",
    "    # Try to create a question\n",
    "    question = f\"What is special about {keyword} in the story?\"\n",
    "    \n",
    "    # Generate distractors\n",
    "    distractors = generate_distractors(keyword)\n",
    "    \n",
    "    # If no distractors found, use generic placeholders\n",
    "    if len(distractors) < 3:\n",
    "        distractors += [\"Magic\", \"Legend\", \"Tree\"]\n",
    "        distractors = distractors[:3]\n",
    "    \n",
    "    # Prepare options\n",
    "    options = [keyword] + distractors\n",
    "    random.shuffle(options)\n",
    "    \n",
    "    # Format options\n",
    "    labels = ['A', 'B', 'C', 'D']\n",
    "    option_texts = [f\"{labels[i]}. {opt}\" for i, opt in enumerate(options)]\n",
    "    \n",
    "    # Print MCQ\n",
    "    print(f\"Q: {question}\")\n",
    "    for opt in option_texts:\n",
    "        print(opt)\n",
    "    print(f\"Answer: {keyword}\\n\")\n",
    "\n",
    "# Main\n",
    "keywords = extract_keywords(text)\n",
    "for kw in keywords:\n",
    "    # Clean keyword\n",
    "    clean_kw = kw.split()[0].capitalize()  # use first word of phrase as keyword\n",
    "    create_mcq(clean_kw, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4db05-8183-4dda-867f-c113e32300b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
